{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPBD Assignment 1\n",
    "\n",
    "This notebook contains the code developed to implement the propoused solutions to this course assignment\n",
    "\n",
    "Developed by:\n",
    "    * Lucas Fischer, nº54659\n",
    "    * Joana Martins, nº54707"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS directories setup\n",
    "\n",
    "The first step is to create directories in the HDFS cluster.\n",
    "1. Create a directory for the group\n",
    "2. Create a directory for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/jovyan/SPBD-1819/Lucas_Joana': File exists\n",
      "mkdir: `/user/jovyan/SPBD-1819/Lucas_Joana/results': File exists\n",
      "Found 3 items\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 19:31 /user/jovyan/SPBD-1819/Lucas_Joana/results/18-11-14-19-30-54\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 19:35 /user/jovyan/SPBD-1819/Lucas_Joana/results/18-11-14-19-35-10\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 19:53 /user/jovyan/SPBD-1819/Lucas_Joana/results/18-11-14-19-52-11\n",
      "Found 6 items\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 18:51 /user/jovyan/SPBD-1819/44987\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 18:34 /user/jovyan/SPBD-1819/Lucas_Joana\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 14:36 /user/jovyan/SPBD-1819/example\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 15:03 /user/jovyan/SPBD-1819/results\n",
      "-rw-r--r--   1 jovyan supergroup      12322 2018-11-14 20:03 /user/jovyan/SPBD-1819/taxi_zone_lookup.csv\n",
      "-rw-r--r--   1 jovyan supergroup  772098307 2018-11-14 01:02 /user/jovyan/SPBD-1819/yellow_tripdata_2018-01.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/jovyan/SPBD-1819/Lucas_Joana\n",
    "!hdfs dfs -mkdir /user/jovyan/SPBD-1819/Lucas_Joana/results\n",
    "!hdfs dfs -ls /user/jovyan/SPBD-1819/Lucas_Joana/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD solution\n",
    "The first implemented solution was to use spark and the RDDs (spark's core abstraction object).\n",
    "This solution creates an inverted index where the key is a given weekday, pick-up zone ID and drop off zone ID and its value is a tuple containing the average of the trip durations, and the average of the trip amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please insert you weekday (1- Monday, 2- Tuesday, ..., 7- Sunday): 1\n",
      "\n",
      "Please insert the desired time (hh:mm): 02:08\n",
      "\n",
      "Please insert you Pick-Up location ID (1 - 265): 246\n",
      "\n",
      "Please insert you Drop-Off location ID (1 - 265): 239\n",
      "Calculating results, please stand by\n",
      "Time to compute : 567.357\n",
      "\n",
      "For monday at 02:08, a trip from (ID: 246) to (ID: 239) takes an average of 8.0 minutes and costs about $12.94111111111111\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import traceback\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import calendar\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]') #Create spark context\n",
    "\n",
    "#PU/DO zone ids range from 1 to 265, see taxi_zone_lookup.csv\n",
    "\n",
    "#Main implementation\n",
    "\n",
    "def get_user_options():\n",
    "    \"\"\"\n",
    "        Function that gets all the users input for creating the inverted index.\n",
    "        This function gets the desired weekday, time, pickup and dropoff zone\n",
    "    \"\"\"\n",
    "\n",
    "    pickup_correct = False\n",
    "    dropoff_correct = False\n",
    "    weekday_correct = False\n",
    "    time_correct = False\n",
    "    pickup_id = \"\"\n",
    "    dropoff_id = \"\"\n",
    "    weekday = \"\"\n",
    "    hour = \"\"\n",
    "    minutes = \"\"                                                \n",
    "\n",
    "    #Continue asking the user until he/she gives us a weekday\n",
    "    while(not weekday_correct):\n",
    "        weekday = input(\"\\nPlease insert you weekday (1- Monday, 2- Tuesday, ..., 7- Sunday): \")\n",
    "        try:\n",
    "            if(int(weekday) >= 1 and int(weekday) <= 7):\n",
    "                weekday_correct = True\n",
    "        except:\n",
    "            #User didn't sent us a number\n",
    "            print(\"\\nPlease insert a number between 1 - 7\\n\")\n",
    "\n",
    "    #Continue asking the user until he/she gives us an hour\n",
    "    while(not time_correct):\n",
    "        time_input = input(\"\\nPlease insert the desired time (hh:mm): \")\n",
    "        try:\n",
    "            user_time = time.strptime(time_input, '%H:%M') # Check time is in proper format\n",
    "            time_correct = True\n",
    "            hour = user_time.tm_hour #Get hour\n",
    "            minutes = user_time.tm_min #Get minutes\n",
    "\n",
    "        except:\n",
    "            #User didn't sent us a number\n",
    "            print(\"\\nPlease insert a time in the format hh:mm where hh (00-23) and mm (00:59) \\n\")    \n",
    "\n",
    "\n",
    "\n",
    "    #Continue asking the user until he/she gives us a number between 1 and 265\n",
    "    while(not pickup_correct):\n",
    "        pickup_id = input(\"\\nPlease insert you Pick-Up location ID (1 - 265): \")\n",
    "        try:\n",
    "            if(int(pickup_id) >= 1 and int(pickup_id) <= 265):\n",
    "                pickup_correct = True\n",
    "        except:\n",
    "            #User didn't sent us a number\n",
    "            print(\"\\nPlease insert a number between 1 - 265\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "    #Continue asking the user until he/she gives us a number between 1 and 265\n",
    "    while(not dropoff_correct):\n",
    "        dropoff_id = input(\"\\nPlease insert you Drop-Off location ID (1 - 265): \")\n",
    "        try:\n",
    "            if(int(dropoff_id) >= 1 and int(dropoff_id) <= 265):\n",
    "                dropoff_correct = True\n",
    "        except:\n",
    "            #User didn't sent us a number\n",
    "            print(\"\\nPlease insert a number between 1 - 265\\n\")\n",
    "\n",
    "\n",
    "    return(weekday, pickup_id, dropoff_id, hour, minutes)\n",
    "\n",
    "\n",
    "def filter_dates(input_date_time, user_weekday, user_hour, user_minutes):\n",
    "    \"\"\"\n",
    "        Predicate function that returns true if input_date_time is within 30 minutes radius of user's desired time, false otherwise\n",
    "\n",
    "        Params:\n",
    "            input_date - String in YYYY-MM-DD HH:MM format\n",
    "            user_weekday - Integer ranging from 1 to 7 representing the weekday\n",
    "            user_hour - Integer representing the hour\n",
    "            user_minutes - Integer representing the minutes\n",
    "\n",
    "        Returns:\n",
    "            True if input_date_time is within 30 minutes radius of user's desired time, false otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    #First check if input_date_time week day is at the maximum one more day than users desired time\n",
    "    date_obj = dt.strptime(input_date_time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    input_weekday = date_obj.weekday()\n",
    "    user_weekday -= 1   #since input_weekday is between [0, 6] we need to subtract 1 to our user_weekday\n",
    "\n",
    "    input_date = input_date_time[0:10] #Getting the characters that represent the date\n",
    "    user_date = dt.strptime(input_date + \" {}:{}:00\".format(user_hour, user_minutes), '%Y-%m-%d %H:%M:%S') #Creating a new date time object with the date of the input date, and time of the user\n",
    "\n",
    "    if(user_hour == 23 and user_minutes > 29):\n",
    "        user_date = dt.strptime(input_date + \" {}:{}:00\".format(user_hour, user_minutes), '%Y-%m-%d %H:%M:%S') - datetime.timedelta(days = 1)\n",
    "\n",
    "    if(input_weekday == user_weekday or (input_weekday == 0 and user_weekday == 6) or (input_weekday == user_weekday + 1)):\n",
    "        time_plus_30_min = (user_date + datetime.timedelta(minutes = 30))\n",
    "        return user_date <= date_obj <= time_plus_30_min\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_key_value(line, user_weekday):\n",
    "    \"\"\"\n",
    "        Function that creates the key value structure for every line of interest\n",
    "\n",
    "        Params:\n",
    "            A non-filtered raw line of the CSV file\n",
    "    \"\"\"\n",
    "    splitted = line.split(\",\")\n",
    "    pick_up_datetime = splitted[1]\n",
    "\n",
    "    week_day = (calendar.day_name[user_weekday - 1]).lower()\n",
    "    hour =  pick_up_datetime[11:13]\n",
    "    minute = pick_up_datetime[14:16]\n",
    "\n",
    "    pick_up_id = splitted[7]\n",
    "    dropoff_up_id = splitted[8]\n",
    "\n",
    "    key = (week_day, pick_up_id, dropoff_up_id)\n",
    "\n",
    "    duration = get_duration(pick_up_datetime,splitted[2])\n",
    "    total_amount = float(splitted[16])\n",
    "    \n",
    "    value = ([duration], [total_amount])\n",
    "\n",
    "    return (key, value)\n",
    "\n",
    "\n",
    "\n",
    "def get_duration(pick_up_datetime, drop_off_datetime):\n",
    "    \"\"\"\n",
    "        Get duration of trip in minutes from pick up and drop off times\n",
    "    \"\"\"\n",
    "\n",
    "    d1 = time.mktime(dt.strptime(drop_off_datetime, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "    d2 = time.mktime(dt.strptime(pick_up_datetime, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "    return int((d1 - d2) / 60)\n",
    "\n",
    "\n",
    "def create_inverted_index(user_weekday = 1, user_puid = 41, user_doid = 24, user_hour = 0, user_minutes = 21, filename = 'hdfs:/user/jovyan/SPBD-1819/yellow_tripdata_2018-01.csv'):\n",
    "    \"\"\"\n",
    "        Function that creates the inverted index. This function holds the main implementation of spark code to create the inverted index\n",
    "        \n",
    "        Params:\n",
    "            user_weekday - An integer ranging from 1 to 7 representing the day of the week chosen by the user\n",
    "            user_puid - An integer ranging from 1 to 265 representing the pick-up zone ID chosen by the user\n",
    "            user_doid - An integer ranging from 1 to 265 representing the drop off zone ID chosen by the user\n",
    "            user_hour - An integer representing the hour chosen by the user\n",
    "            user_hour - An integer representing the minutes chosen by the user\n",
    "            filename - Name of the file to read the information from\n",
    "    \"\"\"\n",
    "    \n",
    "    try :\n",
    "        beforeT = dt.now()\n",
    "        print(\"Calculating results, please stand by\")\n",
    "        lines = sc.textFile(filename) #read csv file (change this to the full dataset instead of just the sample) (this is local to my machine)\n",
    "        first_line = lines.first()\n",
    "\n",
    "        #Filtering out the first line, empty lines\n",
    "        non_empty_lines = lines.filter(lambda line: len(line) > 0 and line != first_line)\n",
    "\n",
    "        #Filter out lines that don't match user's pickup-ID and dropoff-ID\n",
    "        lines_with_piud_doid = non_empty_lines.filter(lambda line: line.split(\",\")[7] == str(user_puid) and line.split(\",\")[8] == str(user_doid))\n",
    "\n",
    "        #Filter out lines that are not within the user's time radius\n",
    "        lines_with_hour = lines_with_piud_doid.filter(lambda line: filter_dates(line.split(\",\")[1], user_weekday, user_hour, user_minutes))\n",
    "\n",
    "        # ((weekday, hour, minute, PU_ID, DO_ID), ([duration], [Total_Ammount]))\n",
    "        organized_lines = lines_with_hour.map(lambda line: create_key_value(line, user_weekday))\n",
    "        \n",
    "        #Reduce everything by key returning a tuple\n",
    "        #(list of durations, list of amounts)\n",
    "        grouped = organized_lines.reduceByKey(lambda accum, elem: (accum[0] + elem[0], accum[1] + elem[1]))\n",
    "        \n",
    "        #Map each of the values to be the mean of the list of durations, and list of ammounts\n",
    "        grouped_with_averages = grouped.mapValues(lambda tup: (np.mean(tup[0]), np.mean(tup[1])))\n",
    "        \n",
    "        grouped_with_averages.saveAsTextFile('hdfs:/user/jovyan/SPBD-1819/Lucas_Joana/results/sparkrdd_' + beforeT.strftime(\"%y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        afterT = datetime.datetime.now()\n",
    "        diffT = afterT - beforeT\n",
    "        print( \"Time to compute : \" + str(diffT.microseconds / 1000))\n",
    "        \n",
    "        sc.stop()\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        sc.stop()\n",
    "\n",
    "\n",
    "\n",
    "user_weekday, user_puid, user_doid, user_hour, user_minutes = get_user_options()\n",
    "\n",
    "create_inverted_index(int(user_weekday), user_puid, user_doid, int(user_hour), int(user_minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop (Map-Reduce)\n",
    "The second implemented solution was to achieve the same goal this time using Hadoop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import calendar\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def filter_dates(input_date_time, user_weekday, user_hour, user_minutes):\n",
    "    \"\"\"\n",
    "        Predicate function that returns true if input_date_time is within 30 minutes radius of user's desired time, false otherwise\n",
    "\n",
    "        Params:\n",
    "            input_date - String in YYYY-MM-DD HH:MM format\n",
    "            user_weekday - Integer ranging from 1 to 7 representing the weekday\n",
    "            user_hour - Integer representing the hour\n",
    "            user_minutes - Integer representing the minutes\n",
    "\n",
    "        Returns:\n",
    "            True if input_date_time is within 30 minutes radius of user's desired time, false otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    #First check if input_date_time week day is at the maximum one more day than users desired time\n",
    "    date_obj = dt.strptime(input_date_time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    input_weekday = date_obj.weekday()\n",
    "    user_weekday -= 1   #since input_weekday is between [0, 6] we need to subtract 1 to our user_weekday\n",
    "\n",
    "    input_date = input_date_time[0:10] #Getting the characters that represent the date\n",
    "    user_date = dt.strptime(input_date + \" {}:{}:00\".format(user_hour, user_minutes), '%Y-%m-%d %H:%M:%S') #Creating a new date time object with the date of the input date, and time of the user\n",
    "\n",
    "    if(user_hour == 23 and user_minutes > 29):\n",
    "        user_date = dt.strptime(input_date + \" {}:{}:00\".format(user_hour, user_minutes), '%Y-%m-%d %H:%M:%S') - datetime.timedelta(days = 1)\n",
    "\n",
    "    if(input_weekday == user_weekday or (input_weekday == 0 and user_weekday == 6) or (input_weekday == user_weekday + 1)):\n",
    "        time_plus_30_min = (user_date + datetime.timedelta(minutes = 30))\n",
    "        return user_date <= date_obj <= time_plus_30_min\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_duration(pick_up_datetime, drop_off_datetime):\n",
    "    \"\"\"\n",
    "        Get duration of trip in minutes from pick up and drop off times\n",
    "    \"\"\"\n",
    "\n",
    "    d1 = time.mktime(dt.strptime(drop_off_datetime, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "    d2 = time.mktime(dt.strptime(pick_up_datetime, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "    return int((d1 - d2) / 60)\n",
    "\n",
    "\n",
    "def create_key_value(splitted_line):\n",
    "    \"\"\"\n",
    "        Function that creates the key value structure for every line of interest\n",
    "\n",
    "        Params:\n",
    "            A non-filtered raw line of the CSV file\n",
    "    \"\"\"\n",
    "    pick_up_datetime = splitted_line[1]\n",
    "\n",
    "    week_day = (calendar.day_name[dt.strptime(splitted_line[1], '%Y-%m-%d %H:%M:%S').weekday()]).lower()\n",
    "    hour =  pick_up_datetime[11:13]\n",
    "    minute = pick_up_datetime[14:16]\n",
    "\n",
    "    pick_up_id = splitted_line[7]\n",
    "    dropoff_up_id = splitted_line[8]\n",
    "\n",
    "    key = (week_day, pick_up_id, dropoff_up_id)\n",
    "\n",
    "    duration = get_duration(pick_up_datetime,splitted_line[2])\n",
    "    total_amount = float(splitted_line[16])\n",
    "    \n",
    "    value = (duration, total_amount)\n",
    "\n",
    "    return (key, value)\n",
    "\n",
    "user_weekday, user_puid, user_doid, user_hour, user_minutes = (1, 246, 239, 2, 8)\n",
    "\n",
    "#Iterating every line from the input file\n",
    "is_first_line = True\n",
    "\n",
    "beforeT = dt.now()\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    if(not is_first_line): #Filtering out the first line\n",
    "        if(len(line) > 0): #Filtering out non empty lines\n",
    "\n",
    "            splitted_line = line.split(\",\")\n",
    "            if(len(splitted_line) == 17):\n",
    "                \n",
    "                if(splitted_line[7] == str(user_puid) and splitted_line[8] == str(user_doid)): #Filtering out lines that don't match pick-up/dropoff ID sent by the user\n",
    "\n",
    "                    if(filter_dates(splitted_line[1], user_weekday, user_hour, user_minutes)): #Filtering out lines that are not between a 30 min radius of the user date and time\n",
    "\n",
    "                        #((Weekday, pick-up ID, drop-off ID), ([duration], [total_amount]))\n",
    "                        key,value = create_key_value(splitted_line)\n",
    "                        \n",
    "                        print(\"{}\\t{}\\t{}\\t{}\".format(key, value[0], value[1], beforeT))\n",
    "                        \n",
    "\n",
    "    else:\n",
    "        is_first_line = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "\n",
    "average_duration = []\n",
    "average_amount = []\n",
    "max_time = 0.0\n",
    "key_name = \"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, duration, amount, beforeT = line.split(\"\\t\")\n",
    "     \n",
    "    if(key_name == \"\"):\n",
    "        key_name = key\n",
    "        \n",
    "    average_duration.append(int(duration))\n",
    "    average_amount.append(float(amount))\n",
    "    \n",
    "    beforeDate = dt.strptime(beforeT.split(\".\")[0], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    afterT = datetime.datetime.now()\n",
    "    diffT = afterT - beforeDate\n",
    "    max_time = max(max_time, (diffT.microseconds / 1000))\n",
    "    \n",
    "print(key_name, (np.mean(average_duration), np.mean(average_amount), max_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py && chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 11:53:09,785 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-11-15 11:53:09,831 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-11-15 11:53:09,831 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-11-15 11:53:09,845 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-15 11:53:09,950 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2018-11-15 11:53:09,961 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2018-11-15 11:53:10,062 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1775675403_0001\n",
      "2018-11-15 11:53:10,063 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-11-15 11:53:10,249 INFO mapred.LocalDistributedCacheManager: Localized file:/home/jovyan/work/mapper.py as file:/tmp/hadoop-jovyan/mapred/local/1542282790140/mapper.py\n",
      "2018-11-15 11:53:10,257 INFO mapred.LocalDistributedCacheManager: Localized file:/home/jovyan/work/reducer.py as file:/tmp/hadoop-jovyan/mapred/local/1542282790141/reducer.py\n",
      "2018-11-15 11:53:10,332 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-11-15 11:53:10,333 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-11-15 11:53:10,334 INFO mapreduce.Job: Running job: job_local1775675403_0001\n",
      "2018-11-15 11:53:10,334 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2018-11-15 11:53:10,337 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-15 11:53:10,337 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-15 11:53:10,357 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-11-15 11:53:10,359 INFO mapred.LocalJobRunner: Starting task: attempt_local1775675403_0001_m_000000_0\n",
      "2018-11-15 11:53:10,377 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-15 11:53:10,377 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-15 11:53:10,389 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-11-15 11:53:10,395 INFO mapred.MapTask: Processing split: file:/home/jovyan/work/yellow_tripdata_2018-01_sample.csv:0+33554432\n",
      "2018-11-15 11:53:10,403 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-11-15 11:53:10,449 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-11-15 11:53:10,449 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-11-15 11:53:10,449 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-11-15 11:53:10,449 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-11-15 11:53:10,449 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-11-15 11:53:10,452 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-11-15 11:53:10,456 INFO streaming.PipeMapRed: PipeMapRed exec [/home/jovyan/work/./mapper.py]\n",
      "2018-11-15 11:53:10,459 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2018-11-15 11:53:10,459 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2018-11-15 11:53:10,460 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2018-11-15 11:53:10,460 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2018-11-15 11:53:10,461 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2018-11-15 11:53:10,461 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2018-11-15 11:53:10,461 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2018-11-15 11:53:10,461 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-11-15 11:53:10,461 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2018-11-15 11:53:10,462 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2018-11-15 11:53:10,462 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-11-15 11:53:10,462 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2018-11-15 11:53:10,476 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:10,477 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:10,478 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:10,484 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:10,656 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:10,782 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:10,905 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,024 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,129 INFO streaming.PipeMapRed: Records R/W=382274/1\n",
      "2018-11-15 11:53:11,145 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-11-15 11:53:11,145 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-11-15 11:53:11,148 INFO mapred.LocalJobRunner: \n",
      "2018-11-15 11:53:11,148 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-11-15 11:53:11,148 INFO mapred.MapTask: Spilling map output\n",
      "2018-11-15 11:53:11,148 INFO mapred.MapTask: bufstart = 0; bufend = 360; bufvoid = 104857600\n",
      "2018-11-15 11:53:11,148 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
      "2018-11-15 11:53:11,152 INFO mapred.MapTask: Finished spill 0\n",
      "2018-11-15 11:53:11,164 INFO mapred.Task: Task:attempt_local1775675403_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-11-15 11:53:11,166 INFO mapred.LocalJobRunner: Records R/W=382274/1\n",
      "2018-11-15 11:53:11,166 INFO mapred.Task: Task 'attempt_local1775675403_0001_m_000000_0' done.\n",
      "2018-11-15 11:53:11,175 INFO mapred.Task: Final Counters for attempt_local1775675403_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=33739972\n",
      "\t\tFILE: Number of bytes written=686153\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=382274\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=360\n",
      "\t\tMap output materialized bytes=378\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=260571136\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=33558528\n",
      "2018-11-15 11:53:11,175 INFO mapred.LocalJobRunner: Finishing task: attempt_local1775675403_0001_m_000000_0\n",
      "2018-11-15 11:53:11,175 INFO mapred.LocalJobRunner: Starting task: attempt_local1775675403_0001_m_000001_0\n",
      "2018-11-15 11:53:11,177 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-15 11:53:11,177 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-15 11:53:11,178 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-11-15 11:53:11,180 INFO mapred.MapTask: Processing split: file:/home/jovyan/work/yellow_tripdata_2018-01_sample.csv:33554432+33554432\n",
      "2018-11-15 11:53:11,183 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-11-15 11:53:11,232 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-11-15 11:53:11,232 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-11-15 11:53:11,232 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-11-15 11:53:11,232 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-11-15 11:53:11,232 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-11-15 11:53:11,233 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-11-15 11:53:11,235 INFO streaming.PipeMapRed: PipeMapRed exec [/home/jovyan/work/./mapper.py]\n",
      "2018-11-15 11:53:11,242 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,242 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,243 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,248 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 11:53:11,338 INFO mapreduce.Job: Job job_local1775675403_0001 running in uber mode : false\n",
      "2018-11-15 11:53:11,339 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2018-11-15 11:53:11,490 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,681 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,815 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:11,953 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,080 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-11-15 11:53:12,080 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-11-15 11:53:12,080 INFO mapred.LocalJobRunner: \n",
      "2018-11-15 11:53:12,080 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-11-15 11:53:12,083 INFO mapred.Task: Task:attempt_local1775675403_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-11-15 11:53:12,084 INFO mapred.LocalJobRunner: file:/home/jovyan/work/yellow_tripdata_2018-01_sample.csv:33554432+33554432\n",
      "2018-11-15 11:53:12,084 INFO mapred.Task: Task 'attempt_local1775675403_0001_m_000001_0' done.\n",
      "2018-11-15 11:53:12,084 INFO mapred.Task: Final Counters for attempt_local1775675403_0001_m_000001_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=67298846\n",
      "\t\tFILE: Number of bytes written=686191\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=381348\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=365953024\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=33558528\n",
      "2018-11-15 11:53:12,084 INFO mapred.LocalJobRunner: Finishing task: attempt_local1775675403_0001_m_000001_0\n",
      "2018-11-15 11:53:12,085 INFO mapred.LocalJobRunner: Starting task: attempt_local1775675403_0001_m_000002_0\n",
      "2018-11-15 11:53:12,086 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-15 11:53:12,086 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-15 11:53:12,086 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-11-15 11:53:12,087 INFO mapred.MapTask: Processing split: file:/home/jovyan/work/yellow_tripdata_2018-01_sample.csv:67108864+20679600\n",
      "2018-11-15 11:53:12,088 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-11-15 11:53:12,131 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-11-15 11:53:12,131 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-11-15 11:53:12,131 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-11-15 11:53:12,131 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-11-15 11:53:12,131 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-11-15 11:53:12,131 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-11-15 11:53:12,134 INFO streaming.PipeMapRed: PipeMapRed exec [/home/jovyan/work/./mapper.py]\n",
      "2018-11-15 11:53:12,142 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,142 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,142 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,143 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,380 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,499 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,644 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-15 11:53:12,705 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-11-15 11:53:12,705 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-11-15 11:53:12,705 INFO mapred.LocalJobRunner: \n",
      "2018-11-15 11:53:12,705 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-11-15 11:53:12,707 INFO mapred.Task: Task:attempt_local1775675403_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-11-15 11:53:12,708 INFO mapred.LocalJobRunner: file:/home/jovyan/work/yellow_tripdata_2018-01_sample.csv:67108864+20679600\n",
      "2018-11-15 11:53:12,708 INFO mapred.Task: Task 'attempt_local1775675403_0001_m_000002_0' done.\n",
      "2018-11-15 11:53:12,709 INFO mapred.Task: Final Counters for attempt_local1775675403_0001_m_000002_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=87978792\n",
      "\t\tFILE: Number of bytes written=686229\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=236378\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=3\n",
      "\t\tTotal committed heap usage (bytes)=503316480\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=20679600\n",
      "2018-11-15 11:53:12,709 INFO mapred.LocalJobRunner: Finishing task: attempt_local1775675403_0001_m_000002_0\n",
      "2018-11-15 11:53:12,709 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-11-15 11:53:12,711 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-11-15 11:53:12,712 INFO mapred.LocalJobRunner: Starting task: attempt_local1775675403_0001_r_000000_0\n",
      "2018-11-15 11:53:12,716 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-15 11:53:12,716 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-15 11:53:12,716 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-11-15 11:53:12,718 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1710a0f0\n",
      "2018-11-15 11:53:12,719 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-15 11:53:12,737 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=1260650496, maxSingleShuffleLimit=315162624, mergeThreshold=832029376, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-11-15 11:53:12,739 INFO reduce.EventFetcher: attempt_local1775675403_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-11-15 11:53:12,757 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1775675403_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "2018-11-15 11:53:12,759 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1775675403_0001_m_000001_0\n",
      "2018-11-15 11:53:12,760 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "2018-11-15 11:53:12,761 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1775675403_0001_m_000000_0 decomp: 374 len: 378 to MEMORY\n",
      "2018-11-15 11:53:12,762 INFO reduce.InMemoryMapOutput: Read 374 bytes from map-output for attempt_local1775675403_0001_m_000000_0\n",
      "2018-11-15 11:53:12,762 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 374, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->376\n",
      "2018-11-15 11:53:12,762 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2018-11-15 11:53:12,762 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1775675403_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n",
      "2018-11-15 11:53:12,763 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1775675403_0001_m_000002_0\n",
      "2018-11-15 11:53:12,764 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 376, usedMemory ->378\n",
      "2018-11-15 11:53:12,764 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2018-11-15 11:53:12,764 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-11-15 11:53:12,764 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-11-15 11:53:12,764 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-11-15 11:53:12,768 INFO mapred.Merger: Merging 3 sorted segments\n",
      "2018-11-15 11:53:12,768 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 347 bytes\n",
      "2018-11-15 11:53:12,769 INFO reduce.MergeManagerImpl: Merged 3 segments, 378 bytes to disk to satisfy reduce memory limit\n",
      "2018-11-15 11:53:12,769 INFO reduce.MergeManagerImpl: Merging 1 files, 378 bytes from disk\n",
      "2018-11-15 11:53:12,770 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-11-15 11:53:12,770 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-11-15 11:53:12,770 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 347 bytes\n",
      "2018-11-15 11:53:12,770 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-11-15 11:53:12,773 INFO streaming.PipeMapRed: PipeMapRed exec [/home/jovyan/work/./reducer.py]\n",
      "2018-11-15 11:53:12,774 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-11-15 11:53:12,775 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-11-15 11:53:12,784 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 11:53:12,938 INFO streaming.PipeMapRed: Records R/W=6/1\n",
      "2018-11-15 11:53:12,952 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-11-15 11:53:12,952 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-11-15 11:53:12,952 INFO mapred.Task: Task:attempt_local1775675403_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-11-15 11:53:12,953 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-11-15 11:53:12,953 INFO mapred.Task: Task attempt_local1775675403_0001_r_000000_0 is allowed to commit now\n",
      "2018-11-15 11:53:12,953 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1775675403_0001_r_000000_0' to file:/home/jovyan/work/results\n",
      "2018-11-15 11:53:12,954 INFO mapred.LocalJobRunner: Records R/W=6/1 > reduce\n",
      "2018-11-15 11:53:12,954 INFO mapred.Task: Task 'attempt_local1775675403_0001_r_000000_0' done.\n",
      "2018-11-15 11:53:12,954 INFO mapred.Task: Final Counters for attempt_local1775675403_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=87979656\n",
      "\t\tFILE: Number of bytes written=686680\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=390\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=6\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=503316480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=73\n",
      "2018-11-15 11:53:12,954 INFO mapred.LocalJobRunner: Finishing task: attempt_local1775675403_0001_r_000000_0\n",
      "2018-11-15 11:53:12,954 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-11-15 11:53:13,342 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-11-15 11:53:13,343 INFO mapreduce.Job: Job job_local1775675403_0001 completed successfully\n",
      "2018-11-15 11:53:13,365 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=276997266\n",
      "\t\tFILE: Number of bytes written=2745253\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1000000\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=360\n",
      "\t\tMap output materialized bytes=390\n",
      "\t\tInput split bytes=327\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=390\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=1633157120\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=87796656\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=73\n",
      "2018-11-15 11:53:13,365 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.1.1/share/hadoop/tools/lib/hadoop-*streaming*.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input yellow_tripdata_2018-01_sample.csv -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 19:31 /user/jovyan/SPBD-1819/Lucas_Joana/results/18-11-14-19-30-54\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-11-14 19:35 /user/jovyan/SPBD-1819/Lucas_Joana/results/18-11-14-19-35-10\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/jovyan/SPBD-1819/Lucas_Joana/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('monday', '246', '238'), ([11, 11], [15.96, 16.56]))\n",
      "(('tuesday', '246', '238'), ([9], [13.3]))\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/jovyan/SPBD-1819/Lucas_Joana/results/18-11-14-19-30-54/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('monday', '246', '239') (8.0, 12.686666666666667, 937.178)\t\r\n"
     ]
    }
   ],
   "source": [
    "!cat results/part-*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
